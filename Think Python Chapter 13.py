import string
import random
'''

p. 125 Chapter 13 Case study: data structure selection

13.1 Word frequency analysis

Exercise 13.1. Write a program that reads a file, breaks each line into words,
strips whitespace and punctuation from the words, and converts them to lowercase.

Hint: The string module provides a string named whitespace, which contains space, tab, newline,
etc., and punctuation which contains the punctuation characters. Let’s see if we can make
Python swear:
>>> import string
>>> string.punctuation
'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
Also, you might consider using the string methods strip, replace and translate.




def whitespaceLowercase(file):
    selection = []
    for line in file:
        lowerCaseWord = line.strip().lower()
        noPunctuation = lowerCaseWord.translate(
            str.maketrans('', '', string.punctuation))
        selection.append(noPunctuation)
    return selection


file = open('Nathaniel Hawthorne - The Scarlet Letter.txt',
            encoding="utf8")        # have to specify encoding, annoying

print(whitespaceLowercase(file))


Exercise 13.2. Go to Project Gutenberg (http: // gutenberg. org ) and download your
favorite out-of-copyright book in plain text format.

Modify your program from the previous exercise to read the book you downloaded, skip
over the header information at the beginning of the file, and process the rest of the
words as before. Then modify the program to count the total number of words in the book,
and the number of times each word is used.

Print the number of different words used in the book. Compare different books by different
authors, written in different eras. Which author uses the most extensive vocabulary?



def whitespaceLowercase(file):
    selection = []
    with open(file) as f:
        linesAfter23 = f.readlines()[23:]
    for line in file:
        lowerCaseWord = line.strip().lower()
        noPunctuation = lowerCaseWord.translate(
            str.maketrans('', '', string.punctuation))
        selection.append(noPunctuation)
    return selection


file = open('Nathaniel Hawthorne - The Scarlet Letter.txt', encoding="utf8")
print(whitespaceLowercase(file))

Exercise 13.3. Modify the program from the previous exercise to print the 20 most
frequently used words in the book.
Exercise 13.4. Modify the previous program to read a word list (see Section 9.1) and
then print all the words in the book that are not in the word list. How many of them are
typos? How many of them are common words that should be in the word list, and how many
of them are really obscure?

p. 126-127 13.2 Random numbers

Given the same inputs, most computer programs generate the same outputs every time,
so they are said to be deterministic. Determinism is usually a good thing, since we expect
the same calculation to yield the same result. For some applications, though, we want the
computer to be unpredictable. Games are an obvious example, but there are more.

Making a program truly nondeterministic turns out to be difficult, but there are ways to
make it at least seem nondeterministic. One of them is to use algorithms that generate
pseudorandom numbers. Pseudorandom numbers are not truly random because they are
generated by a deterministic computation, but just by looking at the numbers it is all but
impossible to distinguish them from random.

The random module provides functions that generate pseudorandom numbers (which I
will simply call “random” from here on).

The function random returns a random float between 0.0 and 1.0 (including 0.0 but not 1.0).
Each time you call random, you get the next number in a long series. To see a sample, run
this loop:

import random
for i in range(10):
    x = random.random()
    print(x)

The function randint takes parameters low and high and returns an integer between low
and high (including both).
>>> random.randint(5, 10)
5
>>> random.randint(5, 10)
9
To choose an element from a sequence at random, you can use choice:

>>> t = [1, 2, 3]
>>> random.choice(t)
2
>>> random.choice(t)
3

The random module also provides functions to generate random values from continuous
distributions including Gaussian, exponential, gamma, and a few more.

Exercise 13.5. Write a function named choose_from_hist that takes a histogram as defined in
Section 11.2 and returns a random value from the histogram, chosen with probability in
proportion to frequency. For example, for this histogram:

>>> t = ['a', 'a', 'b']
>>> hist = histogram(t)
>>> hist
{'a': 2, 'b': 1}

your function should return 'a' with probability 2/3 and 'b' with probability 1/3.



def histogram(s):
    d = dict()
    for c in s:
        if c not in d:
            d[c] = 1
        else:
            d[c] += 1
    return d


def chooseFromHist(hist):
    return random.choice(hist)


t = ['a', 'a', 'b']
hist = histogram(t)
print(chooseFromHist(t))

# Wrong - Answers to exercises 13.1-4

https://github.com/AllenDowney/ThinkPython2/blob/master/code/analyze_book1.py


def process_file(filename, skip_header):
    """Makes a histogram that contains the words from a file.
    filename: string
    skip_header: boolean, whether to skip the Gutenberg header

    returns: map from each word to the number of times it appears.
    """
    hist = {}
    fp = open(filename)

    if skip_header:
        skip_gutenberg_header(fp)

    for line in fp:
        if line.startswith('*** END OF THIS'):
            break

        process_line(line, hist)

    return hist


def skip_gutenberg_header(fp):
    """Reads from fp until it finds the line that ends the header.
    fp: open file object
    """
    for line in fp:
        if line.startswith('*** START OF THIS'):
            break


def process_line(line, hist):
    """Adds the words in the line to the histogram.
    Modifies hist.
    line: string
    hist: histogram (map from word to frequency)
    """
    # TODO: rewrite using Counter

    # replace hyphens with spaces before splitting
    line = line.replace('-', ' ')
    strippables = string.punctuation + string.whitespace

    for word in line.split():
        # remove punctuation and convert to lowercase
        word = word.strip(strippables)
        word = word.lower()

        # update the histogram
        hist[word] = hist.get(word, 0) + 1


def most_common(hist):
    """Makes a list of word-freq pairs in descending order of frequency.
    hist: map from word to frequency
    returns: list of (frequency, word) pairs
    """
    t = []
    for key, value in hist.items():
        t.append((value, key))

    t.sort()
    t.reverse()
    return t


def print_most_common(hist, num=10):
    """Prints the most commons words in a histgram and their frequencies.

    hist: histogram (map from word to frequency)
    num: number of words to print
    """
    t = most_common(hist)
    print('The most common words are:')
    for freq, word in t[:num]:
        print(word, '\t', freq)


def subtract(d1, d2):
    """Returns a dictionary with all keys that appear in d1 but not d2.
    d1, d2: dictionaries
    """
    # TODO: reimplement using Counter
    res = {}
    for key in d1:
        if key not in d2:
            res[key] = None
    return res


def total_words(hist):
    """Returns the total of the frequencies in a histogram."""
    return sum(hist.values())


def different_words(hist):
    """Returns the number of different words in a histogram."""
    return len(hist)


def random_word(hist):
    """Chooses a random word from a histogram.
    The probability of each word is proportional to its frequency.
    """
    # TODO: rewrite using Counter
    t = []
    for word, freq in hist.items():
        t.extend([word] * freq)

    return random.choice(t)


def main():
    hist = process_file('158-0.txt', skip_header=True)
    print('Total number of words:', total_words(hist))
    print('Number of different words:', different_words(hist))

    t = most_common(hist)
    print('The most common words are:')
    for freq, word in t[0:20]:
        print(word, '\t', freq)

    words = process_file('words.txt', skip_header=False)

    diff = subtract(hist, words)
    print("The words in the book that aren't in the word list are:")
    for word in diff.keys():
        print(word, end=' ')

    print("\n\nHere are some random words from the book")
    for i in range(100):
        print(random_word(hist), end=' ')


Here is a program that reads a file and builds a histogram of the words in the file:



def process_file(filename):
    hist = dict()
    fp = open(filename)
    for line in fp:
        process_line(line, hist)
    return hist


def process_line(line, hist):
    line = line.replace('-', ' ')
    for word in line.split():
        word = word.strip(string.punctuation + string.whitespace)
        word = word.lower()
        hist[word] = hist.get(word, 0) + 1


hist = process_file('emma.txt')

This program reads emma.txt, which contains the text of Emma by Jane Austen.

process_file loops through the lines of the file, passing them one at a time to
process_line. The histogram hist is being used as an accumulator.

process_line uses the string method replace to replace hyphens with spaces before using
split to break the line into a list of strings. It traverses the list of words and uses strip
and lower to remove punctuation and convert to lower case. (It is a shorthand to say that
strings are “converted”; remember that strings are immutable, so methods like strip and
lower return new strings.)

Finally, process_line updates the histogram by creating a new item or incrementing an
existing one.

To count the total number of words in the file, we can add up the frequencies in the
histogram:

def total_words(hist):
    return sum(hist.values())

The number of different words is just the number of items in the dictionary:

def different_words(hist):
    return len(hist)

Here is some code to print the results:

print('Total number of words:', total_words(hist))
print('Number of different words:', different_words(hist))

And the results:

Total number of words: 161080
Number of different words: 7214

p. 128 13.4 Most common words

To find the most common words, we can make a list of tuples, where each tuple contains a
word and its frequency, and sort it.

The following function takes a histogram and returns a list of word-frequency tuples:



def most_common(hist):
    t = []
    for key, value in hist.items():
        t.append((value, key))

    t.sort(reverse=True)
    return t


In each tuple, the frequency appears first, so the resulting list is sorted by frequency. Here
is a loop that prints the ten most common words:

t = most_common(hist)

print('The most common words are:')
for freq, word in t[:10]:
    print(word, freq, sep='\t')

I use the keyword argument sep to tell print to use a tab character as a “separator”, rather
than a space, so the second column is lined up. Here are the results from Emma:

The most common words are:
to 5242
the 5205
and 4897
of 4295
i 3191
a 3130
it 2529
her 2483
was 2400
she 2364

This code can be simplified using the key parameter of the sort function. If you are curious,
you can read about it at https://wiki.python.org/moin/HowTo/Sorting.

p. 129 13.5 Optional parameters

We have seen built-in functions and methods that take optional arguments. It is possible
to write programmer-defined functions with optional arguments, too. For example, here is
a function that prints the most common words in a histogram

def print_most_common(hist, num=10):
    t = most_common(hist)
    print('The most common words are:')
    for freq, word in t[:num]:
        print(word, freq, sep='\t')

The first parameter is required; the second is optional. The default value of num is 10.
If you only provide one argument:


print_most_common(hist)

num gets the default value. If you provide two arguments:

print_most_common(hist, 20)

num gets the value of the argument instead. In other words, the optional argument overrides
the default value.

If a function has both required and optional parameters, all the required parameters have
to come first, followed by the optional ones.

p. 129-130 13.6 Dictionary subtraction

Finding the words from the book that are not in the word list from words.txt is a problem
you might recognize as set subtraction; that is, we want to find all the words from one set
(the words in the book) that are not in the other (the words in the list).

subtract takes dictionaries d1 and d2 and returns a new dictionary that contains all the
keys from d1 that are not in d2. Since we don’t really care about the values, we set them all
to None.

def subtract(d1, d2):
    res = dict()
    for key in d1:
        if key not in d2:
            res[key] = None
    return res

To find the words in the book that are not in words.txt, we can use process_file to build
a histogram for words.txt, and then subtract:
words = process_file('words.txt')
diff = subtract(hist, words)

print("Words in the book that aren't in the word list:")
for word in diff:
print(word, end=' ')

Here are some of the results from Emma:

Words in the book that aren't in the word list:
rencontre jane's blanche woodhouses disingenuousness
friend's venice apartment ...

Some of these words are names and possessives. Others, like “rencontre”, are no longer in
common use. But a few are common words that should really be in the list!

Exercise 13.6. Python provides a data structure called set that provides many common set
operations. You can read about them in Section 19.5, or read the documentation at http:
// docs. python. org/ 3/ library/ stdtypes. html# types-set .

Write a program that uses set subtraction to find words in the book that are not in the word list.
Solution: http: // thinkpython2. com/ code/ analyze_ book2. py .

Can't get files to work but it ought to have something like this.

print(setBooks.difference(setFile))

or

set(Books) - set(File)

Answer from book


from analyze_book1 import process_file


def subtract(d1, d2):
    """Returns a set of all keys that appear in d1 but not d2.
    d1, d2: dictionaries
    """
    return set(d1) - set(d2)


def main():
    hist = process_file('158-0.txt', skip_header=True)
    words = process_file('words.txt', skip_header=False)

    diff = subtract(hist, words)
    print("The words in the book that aren't in the word list are:")
    for word in diff:
        print(word, end=' ')


print(main())

p. 130 13.7 Random words

To choose a random word from the histogram, the simplest algorithm is to build a list with
multiple copies of each word, according to the observed frequency, and then choose from
the list:

def random_word(h):
    t = []
    for word, freq in h.items():
        t.extend([word] * freq)
    return random.choice(t)

The expression [word] * freq creates a list with freq copies of the string word. The
extend method is similar to append except that the argument is a sequence.
This algorithm works, but it is not very efficient; each time you choose a random word, it
rebuilds the list, which is as big as the original book. An obvious improvement is to build
the list once and then make multiple selections, but the list is still big.

An alternative is:

1. Use keys to get a list of the words in the book.

2. Build a list that contains the cumulative sum of the word frequencies (see Exercise
10.2). The last item in this list is the total number of words in the book, n.

3. Choose a random number from 1 to n. Use a bisection search (See Exercise 10.10) to
find the index where the random number would be inserted in the cumulative sum.

4. Use the index to find the corresponding word in the word list.

Answer - https://en.wikibooks.org/wiki/Think_Python/Answers#Exercise_13.7


from string import punctuation, whitespace, digits
from random import randint
from bisect import bisect_left

# makes a dictionary out of the file


def process_file(filename):
    h = dict()
    fp = open(filename)
    for line in fp:  # for every line in the dictionary
        process_line(line, h)
    return h

# replaces all punctuation, whitespace and digits as well as makes
# every word lowercase, then turns words into keys that have values
# increasing by one.


def process_line(line, h):
    line = line.replace('-', ' ')
    for word in line.split():
        word = word.strip(punctuation + whitespace + digits)
        word = word.lower()
        if word != '':
            h[word] = h.get(word, 0) + 1


hist = process_file('emma.txt')

# starts with an empty list, then for every value and key in the list, it
# adds the to value if it doesn't exist i.e. 0 or subtracts it once
# every time it's already there


def cum_sum(list_of_numbers):
    cum_list = []
    for i, elem in enumerate(list_of_numbers):
        if i == 0:
            cum_list.append(elem)
        else:
            cum_list.append(cum_list[i - 1] + elem)
    return cum_list


def random_word(h):
    word_list = list(h.keys())
    num_list = []
    for word in word_list:
        num_list.append(h[word])
    cum_list = cum_sum(num_list)
    i = randint(1, cum_list[-1])
    pos = bisect_left(cum_list, i)
    return word_list[pos]


print(random_word(hist))


p. 130-131 13.8 Markov analysis

If you choose words from the book at random, you can get a sense of the vocabulary, but
you probably won’t get a sentence:

Half a bee, philosophically,
Must, ipso facto, half not be.
But half the bee has got to be
Vis a vis, its entity. D’you see?
But can a bee be said to be
Or not to be an entire bee
When half the bee is not a bee
Due to some ancient injury?

In this text, the phrase “half the” is always followed by the word “bee”, but the phrase “the
bee” might be followed by either “has” or “is”.

The result of Markov analysis is a mapping from each prefix (like “half the” and “the bee”)
to all possible suffixes (like “has” and “is”).

Given this mapping, you can generate a random text by starting with any prefix and choosing
at random from the possible suffixes. Next, you can combine the end of the prefix and
the new suffix to form the next prefix, and repeat.

For example, if you start with the prefix “Half a”, then the next word has to be “bee”,
because the prefix only appears once in the text. The next prefix is “a bee”, so the next
suffix might be “philosophically”, “be” or “due”.

In this example the length of the prefix is always two, but you can do Markov analysis with
any prefix length.

Exercise 13.8. Markov analysis:

1. Write a program to read a text from a file and perform Markov analysis. The result should be
a dictionary that maps from prefixes to a collection of possible suffixes. The collection might
be a list, tuple, or dictionary; it is up to you to make an appropriate choice. You can test your
program with prefix length two, but you should write the program in a way that makes it easy
to try other lengths.

2. Add a function to the previous program to generate random text based on the Markov analysis.
Here is an example from Emma with prefix length 2:
He was very clever, be it sweetness or be angry, ashamed or only amused, at such
a stroke. She had never thought of Hannah till you were never meant for me?" "I
cannot make speeches, Emma:" he soon cut it all himself.

For this example, I left the punctuation attached to the words. The result is almost syntactically
correct, but not quite. Semantically, it almost makes sense, but not quite.
What happens if you increase the prefix length? Does the random text make more sense?

3. Once your program is working, you might want to try a mash-up: if you combine text from
two or more books, the random text you generate will blend the vocabulary and phrases from
the sources in interesting ways.

Credit: This case study is based on an example from Kernighan and Pike, The Practice of Programming,
Addison-Wesley, 1999.

You should attempt this exercise before you go on; then you can download my solution
from http://thinkpython2.com/code/markov.py. You will also need http://
thinkpython2.com/code/emma.txt.

Answer below, this chapter is ass.

import sys


# global variables
suffix_map = {}        # map from prefixes to a list of suffixes
prefix = ()            # current tuple of words


def process_file(filename, order=2):
    """Reads a file and performs Markov analysis.
    filename: string
    order: integer number of words in the prefix
    returns: map from prefix to list of possible suffixes.
    """
    fp = open(filename)
    skip_gutenberg_header(fp)

    for line in fp:
        if line.startswith('*** END OF THIS'):
            break

        for word in line.rstrip().split():
            process_word(word, order)


def skip_gutenberg_header(fp):
    """Reads from fp until it finds the line that ends the header.
    fp: open file object
    """
    for line in fp:
        if line.startswith('*** START OF THIS'):
            break


def process_word(word, order=2):
    """Processes each word.
    word: string
    order: integer
    During the first few iterations, all we do is store up the words;
    after that we start adding entries to the dictionary.
    """
    global prefix
    if len(prefix) < order:
        prefix += (word,)
        return

    try:
        suffix_map[prefix].append(word)
    except KeyError:
        # if there is no entry for this prefix, make one
        suffix_map[prefix] = [word]

    prefix = shift(prefix, word)


def random_text(n=100):
    """Generates random words from the analyzed text.
    Starts with a random prefix from the dictionary.
    n: number of words to generate
    """
    # choose a random prefix (not weighted by frequency)
    start = random.choice(list(suffix_map.keys()))

    for i in range(n):
        suffixes = suffix_map.get(start, None)
        if suffixes == None:
            # if the start isn't in map, we got to the end of the
            # original text, so we have to start again.
            random_text(n - i)
            return

        # choose a random suffix
        word = random.choice(suffixes)
        print(word, end=' ')
        start = shift(start, word)


def shift(t, word):
    """Forms a new tuple by removing the head and adding word to the tail.
    t: tuple of strings
    word: string
    Returns: tuple of strings
    """
    return t[1:] + (word,)


def main(script, filename='158-0.txt', n=100, order=2):
    try:
        n = int(n)
        order = int(order)
    except ValueError:
        print('Usage: %d filename [# of words] [prefix length]' % script)
    else:
        process_file(filename, order)
        random_text(n)
        print()

Another answer - https://github.com/dexhunter/TP_solutions/blob/master/ex13_8.py


from collections import defaultdict


def read_and_analyze(filename, skip_header=True):
    Read a text file and perform Markov analysis.
    structure: dict[prefix] = suffix

    Returns a markov dict

    d = defaultdict(list)
    with open(filename, encoding="utf8") as fin:
        if skip_header:
            skip_gutenberg_header(fin)

        for line in fin:
            line = line.replace('-', '')
            line_split = line.split()
            for i in range(0, len(line_split) - 1):
                strippables = string.punctuation + string.whitespace
                word = line_split[i]
                word = word.strip(strippables).lower()
                # simple behavior with low effiency
                d[word].append(line_split[i + 1])
                # This method could have problem with next line
    return d


def generate_random_content(d, prefix_len=2, text_len=50):
    Generate a random text based on a given markov dict

    Starts again if raised error

    des = []
    word_list = []
    for word in list(d.keys()):
        if len(word) == prefix_len:
            word_list.append(word)
    first = random.choice(word_list)
    des.append(first)
    first_s = d[first]
    index2 = random.randint(0, len(first_s) - 1)
    for i in range(text_len):
        previous = first_s[index2]
        sub = d[previous]
        random_index = random.randint(0, len(sub) - 1)
        des.append(sub[random_index])

    print(' '.join(des))


def skip_gutenberg_header(fp):
    Reads from fp until it finds the line that ends the header.
    fp: open file object

    copied from author's answer

    for line in fp:
        if line.startswith('*END*THE SMALL PRINT!'):
            break

def main():

    d = read_and_analyze('emma.txt')
    # for prefix, suffix in d.items():
    #   print(prefix, suffix)
    print("Generating random text")
    generate_random_content(d)


main()

p. 132- Data structures

Using Markov analysis to generate random text is fun, but there is also a point to this
exercise: data structure selection. In your solution to the previous exercises, you had to
choose:

• How to represent the prefixes.
• How to represent the collection of possible suffixes.
• How to represent the mapping from each prefix to the collection of possible suffixes.

The last one is easy: a dictionary is the obvious choice for a mapping from keys to corresponding
values.

For the prefixes, the most obvious options are string, list of strings, or tuple of strings.
For the suffixes, one option is a list; another is a histogram (dictionary).

How should you choose? The first step is to think about the operations you will need to
implement for each data structure. For the prefixes, we need to be able to remove words
from the beginning and add to the end. For example, if the current prefix is “Half a”, and
the next word is “bee”, you need to be able to form the next prefix, “a bee”.

Your first choice might be a list, since it is easy to add and remove elements, but we also
need to be able to use the prefixes as keys in a dictionary, so that rules out lists. With tuples,
you can’t append or remove, but you can use the addition operator to form a new tuple:

def shift(prefix, word):
    return prefix[1:] + (word,)

shift takes a tuple of words, prefix, and a string, word, and forms a new tuple that has
all the words in prefix except the first, and word added to the end.

For the collection of suffixes, the operations we need to perform include adding a new
suffix (or increasing the frequency of an existing one), and choosing a random suffix.
Adding a new suffix is equally easy for the list implementation or the histogram. Choosing
a random element from a list is easy; choosing from a histogram is harder to do efficiently
(see Exercise 13.7).

So far we have been talking mostly about ease of implementation, but there are other factors
to consider in choosing data structures. One is run time. Sometimes there is a theoretical
reason to expect one data structure to be faster than other; for example, I mentioned that
the in operator is faster for dictionaries than for lists, at least when the number of
elements is large.

But often you don’t know ahead of time which implementation will be faster. One option is
to implement both of them and see which is better. This approach is called benchmarking.

A practical alternative is to choose the data structure that is easiest to implement, and then
see if it is fast enough for the intended application. If so, there is no need to go on. If not,
there are tools, like the profile module, that can identify the places in a program that take
the most time.

The other factor to consider is storage space. For example, using a histogram for the collection
of suffixes might take less space because you only have to store each word once, no
matter how many times it appears in the text. In some cases, saving space can also make
your program run faster, and in the extreme, your program might not run at all if you run
out of memory. But for many applications, space is a secondary consideration after run
time.

One final thought: in this discussion, I have implied that we should use one data structure
for both analysis and generation. But since these are separate phases, it would also be possible
to use one structure for analysis and then convert to another structure for generation.
This would be a net win if the time saved during generation exceeded the time spent in
conversion.

p. 133-134 13.10 Debugging

When you are debugging a program, and especially if you are working on a hard bug,
there are five things to try:

Reading: Examine your code, read it back to yourself, and check that it says what you
meant to say.

Running: Experiment by making changes and running different versions. Often if you
display the right thing at the right place in the program, the problem becomes obvious,
but sometimes you have to build scaffolding.

Ruminating: Take some time to think! What kind of error is it: syntax, runtime, or semantic?
What information can you get from the error messages, or from the output of the
program? What kind of error could cause the problem you’re seeing? What did you
change last, before the problem appeared?

Rubberducking: If you explain the problem to someone else, you sometimes find the
answer before you finish asking the question. Often you don’t need the other
person; you could just talk to a rubber duck. And that’s the origin of the wellknown
strategy called rubber duck debugging. I am not making this up; see
https://en.wikipedia.org/wiki/Rubber_duck_debugging.

Retreating: At some point, the best thing to do is back off, undoing recent changes, until
you get back to a program that works and that you understand. Then you can start
rebuilding.

p. 134-135 13.12 Exercises

Exercise 13.9. The “rank” of a word is its position in a list of words sorted by frequency: the most
common word has rank 1, the second most common has rank 2, etc.

Zipf’s law describes a relationship between the ranks and frequencies of words in natural languages
(http: // en. wikipedia. org/ wiki/ Zipf's_ law ). Specifically, it predicts that the frequency,
f , of the word with rank r is:

f = cr􀀀s

where s and c are parameters that depend on the language and the text. If you take the logarithm of
both sides of this equation, you get:

log f = log c 􀀀 s log r

So if you plot log f versus log r, you should get a straight line with slope 􀀀s and intercept log c.
Write a program that reads a text from a file, counts word frequencies, and prints one line for each
word, in descending order of frequency, with log f and log r. Use the graphing program of your
choice to plot the results and check whether they form a straight line. Can you estimate the value of
s?

Solution: http: // thinkpython2. com/ code/ zipf. py . To run my solution, you need the plotting
module matplotlib. If you installed Anaconda, you already have matplotlib; otherwise you
might have to install it.

Answer, this shit is way over my head.
'''

import sys

import matplotlib.pyplot as plt

from analyze_book1 import process_file


def rank_freq(hist):
    """Returns a list of (rank, freq) tuples.
    hist: map from word to frequency
    returns: list of (rank, freq) tuples
    """
    # sort the list of frequencies in decreasing order
    freqs = list(hist.values())
    freqs.sort(reverse=True)

    # enumerate the ranks and frequencies
    rf = [(r + 1, f) for r, f in enumerate(freqs)]
    return rf


def print_ranks(hist):
    """Prints the rank vs. frequency data.
    hist: map from word to frequency
    """
    for r, f in rank_freq(hist):
        print(r, f)


def plot_ranks(hist, scale='log'):
    """Plots frequency vs. rank.
    hist: map from word to frequency
    scale: string 'linear' or 'log'
    """
    t = rank_freq(hist)
    rs, fs = zip(*t)

    plt.clf()
    plt.xscale(scale)
    plt.yscale(scale)
    plt.title('Zipf plot')
    plt.xlabel('rank')
    plt.ylabel('frequency')
    plt.plot(rs, fs, 'r-', linewidth=3)
    plt.show()


def main(script, filename='emma.txt', flag='plot'):
    hist = process_file(filename, skip_header=True)

    # either print the results or plot them
    if flag == 'print':
        print_ranks(hist)
    elif flag == 'plot':
        plot_ranks(hist)
    else:
        print('Usage: zipf.py filename [print|plot]')


main()
